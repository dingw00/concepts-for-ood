{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17493425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingw/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
      "2024-07-23 19:54:27.194379: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-23 19:54:27.283848: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-23 19:54:27.312173: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-23 19:54:27.480385: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-23 19:54:28.593145: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 19:54:29.546271: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2024-07-23 19:54:29.546329: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:135] retrieving CUDA diagnostic information for host: trieves\n",
      "2024-07-23 19:54:29.546334: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:142] hostname: trieves\n",
      "2024-07-23 19:54:29.546412: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:166] libcuda reported version is: 555.42.6\n",
      "2024-07-23 19:54:29.546428: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] kernel reported version is: NOT_FOUND: could not find kernel module information in driver version file contents: \"NVRM version: NVIDIA UNIX Open Kernel Module for x86_64  555.42.06  Release Build  (dvs-builder@U16-I3-A13-3-4)  Tue Jun  4 00:45:31 UTC 2024\n",
      "GCC version:  gcc version 12.3.0 (Ubuntu 12.3.0-1ubuntu1~22.04) \n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import itertools\n",
    "import copy\n",
    "import pandas as pd\n",
    "from scipy.special import comb\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sns.set_style(\"whitegrid\") #darkgrid\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "SMALL_SIZE=12\n",
    "MEDIUM_SIZE=15\n",
    "BIGGER_SIZE=20\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "#plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "#plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow.keras.metrics as metrics\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass\n",
    "print(physical_devices)\n",
    "import concept_model\n",
    "import helper\n",
    "# from test_baselines import run_eval\n",
    "\n",
    "\n",
    "from utils.test_utils import arg_parser, prepare_data, get_measures\n",
    "from utils.test_utils import ConceptProfiles\n",
    "from utils.test_utils import get_recovered_features\n",
    "from utils.ood_utils import run_ood_over_batch\n",
    "from utils.stat_utils import hellinger, compute_pval, bayes_posterior, FLD, multivar_separa\n",
    "from utils.plot_utils import plot_stats, plot_per_class_stats, plot_score_distr\n",
    "from utils import log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03108c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.in_data = \"data/AwA2/test\"\n",
    "        self.out_data = \"MSCOCO\"\n",
    "        self.workers = 4\n",
    "        \n",
    "        self.batch_size = 256\n",
    "        self.name = \"test\"\n",
    "        self.model = \"InceptionV3\"\n",
    "        self.model_path = \"results/AwA2/inceptionv3_AwA2_e20.weights.h5\"\n",
    "\n",
    "        self.gpu = \"0\"\n",
    "        self.result_dir = \"results/AwA2_1_baseline_s0/epoch_20\"\n",
    "        self.logdir = self.result_dir+\"/logs\"\n",
    "        \n",
    "        self.visualize = True\n",
    "        self.visualize_with_ood = True\n",
    "        self.shap = True\n",
    "        self.separate = True\n",
    "        self.explain = True\n",
    "        self.plot = True\n",
    "        self.out_data_dim = 224\n",
    "        self.score = \"Energy\"\n",
    "        self.temperature_energy = 1\n",
    "\n",
    "        self.opt = \"adam\"\n",
    "\n",
    "args = ARGS()\n",
    "softmax = layers.Activation('softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fee29317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_concepts(topic_vec, return_mapping=False):\n",
    "    # Remove one concept vector if there are two vectors where the dot product is over 0.95\n",
    "    # topic_vec: dim=(dim_features, n_concepts) (2048, 70)\n",
    "    # print(np.shape(topic_vec))\n",
    "    n_concept = topic_vec.shape[1]\n",
    "    thresh = 0.95\n",
    "    topic_vec_n = topic_vec/(np.linalg.norm(topic_vec,axis=0,keepdims=True)+1e-9)\n",
    "\n",
    "    topic_vec_n_dot = np.transpose(topic_vec_n) @ topic_vec_n - np.eye(n_concept)\n",
    "    dict_similar_topic = {}\n",
    "    idx_delete = set()\n",
    "    for i in range(n_concept):\n",
    "        ith_redundant_concepts = [j for j in range(n_concept) if topic_vec_n_dot[i][j] >= 0.95]\n",
    "        dict_similar_topic[i] = ith_redundant_concepts\n",
    "        \n",
    "        ith_redundant_concepts = [x for x in ith_redundant_concepts if x > i]\n",
    "        idx_delete.update(ith_redundant_concepts)\n",
    "    idx_delete = list(idx_delete)\n",
    "\n",
    "    print(dict_similar_topic)\n",
    "    print(idx_delete)\n",
    "\n",
    "    topic_vec_r = np.delete(topic_vec, idx_delete, axis=1)\n",
    "\n",
    "\n",
    "    dict_topic_mapping = {}\n",
    "    count = 0\n",
    "    for i in range(n_concept):\n",
    "        if i in idx_delete:\n",
    "            dict_topic_mapping[i] = None\n",
    "        else:\n",
    "            dict_topic_mapping[i] = count\n",
    "            count += 1\n",
    "    print('concept mapping between before/after duplicate removal......')\n",
    "    print(dict_topic_mapping)\n",
    "    if return_mapping:\n",
    "        return topic_vec_r, dict_similar_topic, dict_topic_mapping\n",
    "    else:\n",
    "        return topic_vec_r, dict_similar_topic\n",
    "    \n",
    "def compute_concept_scores(topic_vec, feature, predict_model=None):\n",
    "    # topic_vec: concept vectors (dim= (feature_dim, n_concepts))\n",
    "    # feature: features extracted from an intermediate layer of trained model\n",
    "\n",
    "    feature_n = tf.math.l2_normalize(feature, axis=3)\n",
    "    topic_vec_n = tf.math.l2_normalize(topic_vec, axis=0)\n",
    "\n",
    "    topic_prob = tf.matmul(feature_n, topic_vec_n) # K.dot\n",
    "\n",
    "    prob_max = tf.math.reduce_max(topic_prob, axis=(1,2))\n",
    "    prob_max_abs = tf.math.reduce_max(tf.abs(topic_prob), axis=(1,2))\n",
    "    concept_scores = tf.where(prob_max == prob_max_abs, prob_max, -prob_max_abs)\n",
    "\n",
    "    \"\"\"\n",
    "    ##for debugging\n",
    "    n_concept = np.shape(concept_scores)[1]\n",
    "    print(tf.reduce_mean(input_tensor=tf.nn.top_k(K.transpose(K.reshape(topic_prob,(-1,n_concept))),k=10,sorted=True).values))\n",
    "    print(tf.reduce_mean(input_tensor=K.dot(K.transpose(K.variable(value=topic_vec_n)), K.variable(value=topic_vec_n)) - np.eye(n_concept)))\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if predict_model: # in eager execution\n",
    "        pred = softmax(predict_model(feature))\n",
    "        #pred = tf.math.argmax(pred, axis=1)\n",
    "        return concept_scores.numpy(), pred.numpy()\n",
    "    else:\n",
    "        return concept_scores\n",
    "\n",
    "def compute_completeness(y, yhat, yhat_recov, num_class, logger=None, label=None):\n",
    "    \"\"\"\n",
    "    compute completeness score by Yeh et al.\n",
    "    :param y: groundtruth class labels, dim=(N,)\n",
    "    :param yhat: predicted class labels, dim=(N,)\n",
    "    :param yhat_recov: predicted class labels using recovered features, dim=(N,).\n",
    "                       If label is not None, per-class predicted labels, dim=(N',) where N' <= N\n",
    "    \"\"\"\n",
    "\n",
    "    acc = np.sum(y == yhat)/len(y)\n",
    "    if logger:\n",
    "        logger.info(f'[ID TEST] accuracy with original features: {acc}')\n",
    "    \n",
    "    if label is not None:\n",
    "        acc_recov = np.sum(y[y==label] == yhat_recov)/len(yhat_recov)\n",
    "        if logger:\n",
    "            logger.info(f'[ID TEST] per-class accuracy with recovered features: {acc_recov}')\n",
    "        acc_random = 1/num_class #0.5 #NOTE: check a_r = 0.5?\n",
    "    else:\n",
    "        acc_recov = np.sum(y == yhat_recov)/len(y)\n",
    "        if logger:\n",
    "            logger.info(f'[ID TEST] accuracy with recovered features: {acc_recov}')\n",
    "        acc_random = 1/num_class\n",
    "    \n",
    "    # compute completeness\n",
    "    completeness = (acc_recov - acc_random) / (acc - 1/num_class)\n",
    "    if logger:\n",
    "        logger.info(f'[ID TEST] completeness score: {completeness}')\n",
    "    return completeness\n",
    "\n",
    "def compute_detection_completeness(auroc, auroc_recov, logger=None):\n",
    "    \"\"\"\n",
    "    compute detection completeness score\n",
    "    \"\"\"\n",
    "    # compute completeness\n",
    "    auroc_random = 1/2\n",
    "    completeness = (auroc_recov - auroc_random) / (auroc - auroc_random)\n",
    "    if logger:\n",
    "        logger.info(f'[DETECTION] auroc with original features: {auroc}')\n",
    "        logger.info(f'[DETECTION] auroc with recovered features: {auroc_recov}')\n",
    "        logger.info(f'[DETECTION] completeness score: {completeness}')\n",
    "    return completeness\n",
    "\n",
    "def run_eval(feature_model, predict_model, in_loader, out_loader, logger, args, num_classes):\n",
    "    in_scores = np.array([])\n",
    "    for i, (x, y) in tqdm(enumerate(in_loader)):\n",
    "        if i == len(in_loader):\n",
    "            break\n",
    "        score = run_ood_over_batch(x, feature_model, predict_model, args, num_classes).numpy()\n",
    "        in_scores = np.concatenate([in_scores, score])\n",
    "    out_scores = np.array([])\n",
    "    for i, x in tqdm(enumerate(out_loader)):\n",
    "        if i == len(in_loader):\n",
    "            break\n",
    "        score = run_ood_over_batch(x, feature_model, predict_model, args, num_classes).numpy()\n",
    "        out_scores = np.concatenate([out_scores, score])\n",
    "    in_examples = np.expand_dims(in_scores, axis=1)\n",
    "    out_examples = np.expand_dims(out_scores, axis=1)\n",
    "    auroc, aupr_in, aupr_out, fpr, thres95 = get_measures(in_examples, out_examples)\n",
    "    return in_scores, out_scores, auroc, fpr, thres95\n",
    "\n",
    "def get_class_labels(loader, savepath):\n",
    "    \"\"\"\n",
    "    extract groundtruth class labels from data loader\n",
    "    :param loader: data loader\n",
    "    :param savepath: path to the numpy file\n",
    "    \"\"\"\n",
    "    if os.path.exists(savepath):\n",
    "        y = np.load(savepath)\n",
    "    else:\n",
    "        num_data = len(loader.filenames)\n",
    "        y = []\n",
    "        for (_, y_batch), _ in zip(loader, range(len(loader))):\n",
    "            y.extend(y_batch)\n",
    "       \n",
    "        np.save(savepath, y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8f81a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 19:54:29,958 [INFO] utils.log: <__main__.ARGS object at 0x700a58253e20>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3772 images belonging to 50 classes.\n",
      "Found 40670 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 19:54:30,779 [INFO] utils.log: Using an in-distribution set.\n",
      "2024-07-23 19:54:30,779 [INFO] utils.log: Using an out-of-distribution set.\n",
      "2024-07-23 19:54:30,780 [INFO] utils.log: Loading model from results/AwA2/inceptionv3_AwA2_e20.weights.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "original model to be trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingw/.local/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:418: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling2d            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,850</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_2 (\u001b[38;5;33mInputLayer\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling2d            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m524,544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m12,850\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">537,394</span> (2.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m537,394\u001b[0m (2.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">537,394</span> (2.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m537,394\u001b[0m (2.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingw/.local/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 8s/step\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "\n",
    "# with tf.device('/CPU:0'):\n",
    "\n",
    "logger = log.setup_logger(args, filename=\"eval_{}.log\".format(args.score))\n",
    "LOAD_DIR = 'data/AwA2'\n",
    "TOPIC_PATH = os.path.join(args.result_dir,'topic_vec_inceptionv3.npy')\n",
    "INPUT_SHAPE = (args.out_data_dim, args.out_data_dim)\n",
    "TRAIN_DIR = \"data/AwA2/train\"\n",
    "N_CLASSES = 50\n",
    "N_CONCEPTS_ORIG = 100 #np.shape(topic_vec_orig)[-1]\n",
    "_ = 0\n",
    "\n",
    "if args.score == 'ODIN':\n",
    "    args.batch_size = 200\n",
    "\n",
    "if not os.path.exists(os.path.join(args.result_dir, 'plots')):\n",
    "    os.makedirs(os.path.join(args.result_dir, 'plots'))\n",
    "if not os.path.exists(os.path.join(args.result_dir, 'explanations')):\n",
    "    os.makedirs(os.path.join(args.result_dir, 'explanations'))\n",
    "if not os.path.exists(os.path.join(args.result_dir, 'explanations', args.out_data+'_'+args.score)):\n",
    "    os.makedirs(os.path.join(args.result_dir, 'explanations', args.out_data+'_'+args.score))\n",
    "explain_dir = os.path.join(args.result_dir, 'explanations', args.out_data+'_'+args.score)\n",
    "\n",
    "in_loader, out_loader = prepare_data(args, logger)\n",
    "\n",
    "## load trained_model\n",
    "logger.info(f\"Loading model from {args.model_path}\")\n",
    "feature_model, predict_model = helper.load_model_inception_new(_, in_loader, batch_size=args.batch_size, \n",
    "                                    input_size=INPUT_SHAPE, pretrain=True, modelname=args.model_path)\n",
    "\n",
    "in_test_features = feature_model.predict(in_loader)\n",
    "N_IN = in_test_features.shape[0]\n",
    "# out_test_features = feature_model.predict(out_loader, steps=len(in_loader))\n",
    "# N_OUT = out_test_features.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "add30f49-65c0-4576-a101-9869005355fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 19:56:45,012 [INFO] utils.log: Number of concepts before removing duplicate ones: 100\n",
      "2024-07-23 19:56:45,020 [INFO] utils.log: Number of concepts after removing duplicate ones: 35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [4, 24, 43, 45, 61, 69, 73, 74, 97], 1: [5, 17, 28, 32, 33, 34, 78, 82], 2: [8, 16, 27, 29, 36, 50, 68, 92, 94, 95, 96], 3: [14, 30, 44], 4: [0, 24, 43, 45, 61, 69, 73, 74, 97], 5: [1, 17, 28, 32, 33, 34, 78, 82], 6: [39, 41, 49, 66, 88], 7: [], 8: [2, 16, 27, 29, 36, 50, 68, 92, 94, 95, 96], 9: [], 10: [], 11: [22, 23, 91], 12: [84], 13: [42], 14: [3, 30, 44], 15: [20, 40, 75], 16: [2, 8, 27, 29, 36, 50, 68, 92, 94, 95, 96], 17: [1, 5, 28, 32, 33, 34, 78, 82], 18: [87], 19: [47, 60, 64, 77], 20: [15, 40, 75], 21: [56, 58, 81], 22: [11, 23], 23: [11, 22, 91], 24: [0, 4, 43, 45, 61, 69, 73, 74, 97], 25: [65, 85], 26: [], 27: [2, 8, 16, 29, 36, 50, 68, 92, 94, 95, 96], 28: [1, 5, 17, 32, 33, 34, 78, 82], 29: [2, 8, 16, 27, 36, 50, 68, 92, 94, 95, 96], 30: [3, 14, 44], 31: [], 32: [1, 5, 17, 28, 33, 34, 78, 82], 33: [1, 5, 17, 28, 32, 34, 78, 82], 34: [1, 5, 17, 28, 32, 33, 78, 82], 35: [], 36: [2, 8, 16, 27, 29, 50, 68, 92, 94, 95, 96], 37: [76], 38: [52, 55], 39: [6, 41, 49, 66, 88, 98], 40: [15, 20, 75], 41: [6, 39, 49, 66, 88, 98], 42: [13], 43: [0, 4, 24, 45, 61, 69, 73, 74, 97], 44: [3, 14, 30], 45: [0, 4, 24, 43, 61, 69, 73, 74, 97], 46: [], 47: [19, 60, 64, 77], 48: [], 49: [6, 39, 41, 66, 88, 98], 50: [2, 8, 16, 27, 29, 36, 68, 92, 94, 95, 96], 51: [], 52: [38, 55], 53: [54], 54: [53], 55: [38, 52], 56: [21, 58, 81], 57: [70, 99], 58: [21, 56, 81], 59: [71, 90], 60: [19, 47, 64, 77], 61: [0, 4, 24, 43, 45, 69, 73, 74, 97], 62: [], 63: [], 64: [19, 47, 60, 77], 65: [25, 85], 66: [6, 39, 41, 49, 88, 98], 67: [86], 68: [2, 8, 16, 27, 29, 36, 50, 92, 94, 95, 96], 69: [0, 4, 24, 43, 45, 61, 73, 74, 97], 70: [57, 99], 71: [59, 90], 72: [], 73: [0, 4, 24, 43, 45, 61, 69, 74, 97], 74: [0, 4, 24, 43, 45, 61, 69, 73, 97], 75: [15, 20, 40], 76: [37], 77: [19, 47, 60, 64], 78: [1, 5, 17, 28, 32, 33, 34, 82], 79: [], 80: [93], 81: [21, 56, 58], 82: [1, 5, 17, 28, 32, 33, 34, 78], 83: [], 84: [12], 85: [25, 65], 86: [67], 87: [18], 88: [6, 39, 41, 49, 66, 98], 89: [], 90: [59, 71], 91: [11, 23], 92: [2, 8, 16, 27, 29, 36, 50, 68, 94, 95, 96], 93: [80], 94: [2, 8, 16, 27, 29, 36, 50, 68, 92, 95, 96], 95: [2, 8, 16, 27, 29, 36, 50, 68, 92, 94, 96], 96: [2, 8, 16, 27, 29, 36, 50, 68, 92, 94, 95], 97: [0, 4, 24, 43, 45, 61, 69, 73, 74], 98: [39, 41, 49, 66, 88], 99: [57, 70]}\n",
      "[4, 5, 8, 14, 16, 17, 20, 22, 23, 24, 27, 28, 29, 30, 32, 33, 34, 36, 39, 40, 41, 42, 43, 44, 45, 47, 49, 50, 52, 54, 55, 56, 58, 60, 61, 64, 65, 66, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 81, 82, 84, 85, 86, 87, 88, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
      "concept mapping between before/after duplicate removal......\n",
      "{0: 0, 1: 1, 2: 2, 3: 3, 4: None, 5: None, 6: 4, 7: 5, 8: None, 9: 6, 10: 7, 11: 8, 12: 9, 13: 10, 14: None, 15: 11, 16: None, 17: None, 18: 12, 19: 13, 20: None, 21: 14, 22: None, 23: None, 24: None, 25: 15, 26: 16, 27: None, 28: None, 29: None, 30: None, 31: 17, 32: None, 33: None, 34: None, 35: 18, 36: None, 37: 19, 38: 20, 39: None, 40: None, 41: None, 42: None, 43: None, 44: None, 45: None, 46: 21, 47: None, 48: 22, 49: None, 50: None, 51: 23, 52: None, 53: 24, 54: None, 55: None, 56: None, 57: 25, 58: None, 59: 26, 60: None, 61: None, 62: 27, 63: 28, 64: None, 65: None, 66: None, 67: 29, 68: None, 69: None, 70: None, 71: None, 72: 30, 73: None, 74: None, 75: None, 76: None, 77: None, 78: None, 79: 31, 80: 32, 81: None, 82: None, 83: 33, 84: None, 85: None, 86: None, 87: None, 88: None, 89: 34, 90: None, 91: None, 92: None, 93: None, 94: None, 95: None, 96: None, 97: None, 98: None, 99: None}\n"
     ]
    }
   ],
   "source": [
    "## load topic model\n",
    "topic_model = concept_model.TopicModel(in_test_features, N_CONCEPTS_ORIG, thres=0.0, predict=predict_model)\n",
    "topic_model(in_test_features)\n",
    "topic_model.load_weights(os.path.dirname(args.result_dir)+'/topic_epoch20.weights.h5')\n",
    "for layer in topic_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "topic_vec_orig = topic_model.layers[0].get_weights()[0]\n",
    "np.save(args.result_dir+'/topic_vec_orig.npy', topic_vec_orig)\n",
    "logger.info(f'Number of concepts before removing duplicate ones: {str(N_CONCEPTS_ORIG)}')\n",
    "\n",
    "topic_vec, dict_dupl_topic = remove_duplicate_concepts(topic_vec_orig)\n",
    "N_CONCEPTS = np.shape(topic_vec)[-1] # 25\n",
    "logger.info(f'Number of concepts after removing duplicate ones: {str(N_CONCEPTS)}')\n",
    "\n",
    "in_test_concepts, in_test_logits = compute_concept_scores(topic_vec, in_test_features, predict_model)\n",
    "in_test_yhat = np.argmax(in_test_logits, axis=1) \n",
    "\n",
    "# out_test_concepts, out_test_logits = compute_concept_scores(topic_vec, out_test_features, predict_model)\n",
    "# out_test_yhat = np.argmax(out_test_logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a51a599e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 19:56:47,012 [INFO] utils.log: [ID TEST] performance of target OOD detector with test set...\n",
      "2024-07-23 19:56:55,065 [INFO] utils.log: [ID TEST] accuracy with original features: 0.8820254506892895\n",
      "2024-07-23 19:56:55,066 [INFO] utils.log: [ID TEST] accuracy with recovered features: 0.8515376458112407\n",
      "2024-07-23 19:56:55,066 [INFO] utils.log: [ID TEST] completeness score: 0.9646323610820652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9646323610820652"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target OOD detector\n",
    "logger.info(\"[ID TEST] performance of target OOD detector with test set...\")\n",
    "\n",
    "## Evaluating the difference between two worlds......\n",
    "y_test = np.argmax(np.load('data/AwA2/y_test.npy'), axis=1) # true labels\n",
    "\n",
    "# compute completeness scores\n",
    "_, logits_recov, _ = topic_model(in_test_features)\n",
    "in_test_yhat_recov = tf.math.argmax(logits_recov, axis=1).numpy()\n",
    "compute_completeness(y_test, in_test_yhat, in_test_yhat_recov, N_CLASSES, logger)\n",
    "\n",
    "# in_test_scores, out_test_scores, auroc, fpr, thres95 = run_eval(feature_model, predict_model, in_loader, out_loader, logger, args, N_CLASSES)\n",
    "# in_test_scores, out_test_scores, thres95, auroc = np.random.rand(N_IN), np.random.rand(N_OUT), 0.5419758558273315, 0.955332290562036\n",
    "# compute_detection_completeness(auroc, auroc_recov, logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fad90b71-69a7-4163-863e-1686f9b5ef41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images through generators ...\n",
      "Found 29841 images belonging to 50 classes.\n",
      "Found 3709 images belonging to 50 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 19:56:55,940 [INFO] utils.log: \n",
      "[INFO] starting epoch 0/20 ---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[ID] CE': <tf.Tensor 'Mean:0' shape=() dtype=float32>}\n",
      "[<KerasVariable shape=(2048, 100), dtype=float32, path=topic_model/weight/proj>, <KerasVariable shape=(100, 500), dtype=float32, path=topic_model/weight_1/proj>, <KerasVariable shape=(500, 2048), dtype=float32, path=topic_model/weight_2/proj>]\n",
      "{'[ID] CE': <tf.Tensor 'Mean:0' shape=() dtype=float32>}\n",
      "[<KerasVariable shape=(2048, 100), dtype=float32, path=topic_model/weight/proj>, <KerasVariable shape=(100, 500), dtype=float32, path=topic_model/weight_1/proj>, <KerasVariable shape=(500, 2048), dtype=float32, path=topic_model/weight_2/proj>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 19:57:11.770183: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 5242880000 exceeds 10% of free system memory.\n",
      "2024-07-23 19:57:27.294845: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 5242880000 exceeds 10% of free system memory.\n",
      "2024-07-23 19:57:42.731076: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 5242880000 exceeds 10% of free system memory.\n",
      "2024-07-23 19:58:00.585391: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 5242880000 exceeds 10% of free system memory.\n",
      "2024-07-23 19:58:14.021886: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 5242880000 exceeds 10% of free system memory.\n",
      "2024-07-23 20:01:58,162 [INFO] utils.log: [STEP20] [ID] CE: 0.6269979476928711\n",
      "2024-07-23 20:06:40,247 [INFO] utils.log: [STEP40] [ID] CE: 0.41479039192199707\n",
      "2024-07-23 20:10:48,185 [INFO] utils.log: [STEP60] [ID] CE: 0.5814191699028015\n",
      "2024-07-23 20:15:01,371 [INFO] utils.log: [STEP80] [ID] CE: 0.46643754839897156\n"
     ]
    }
   ],
   "source": [
    "### Compute completeness\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TRAIN_DIR = \"data/AwA2/train\"\n",
    "VAL_DIR = \"data/AwA2/val\"\n",
    "\n",
    "print('Loading images through generators ...')\n",
    "datagen = ImageDataGenerator(rescale=1. / 255.)\n",
    "train_loader = datagen.flow_from_directory(TRAIN_DIR,\n",
    "                                            batch_size=args.batch_size,\n",
    "                                            target_size=(224, 224),\n",
    "                                            class_mode='categorical',\n",
    "                                            shuffle=True)\n",
    "val_loader = datagen.flow_from_directory(VAL_DIR,\n",
    "                                        batch_size=args.batch_size,\n",
    "                                        target_size=(224, 224),\n",
    "                                        class_mode='categorical',\n",
    "                                        shuffle=False)\n",
    "\n",
    "y_val_ = np.load('data/AwA2/y_val.npy')\n",
    "y_test_ = np.load('data/AwA2/y_test.npy')\n",
    "\n",
    "if args.opt =='sgd':\n",
    "    \"\"\"\n",
    "    optimizer = SGD(lr=0.1)\n",
    "    optimizer_state = [optimizer.iterations, optimizer.lr, optimizer.momentum, optimizer.decay]\n",
    "    optimizer_reset = tf.compat.v1.variables_initializer(optimizer_state)\n",
    "    \"\"\"\n",
    "    optimizer = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "elif args.opt =='adam':\n",
    "    optimizer = Adam(learning_rate=0.01)\n",
    "    optimizer_state = [optimizer.iterations, optimizer.learning_rate, optimizer.beta_1, optimizer.beta_2, optimizer.weight_decay]\n",
    "    optimizer_reset = tf.compat.v1.variables_initializer(optimizer_state)\n",
    "\n",
    "train_acc_metric = metrics.CategoricalAccuracy()\n",
    "val_acc_metric = metrics.CategoricalAccuracy()\n",
    "test_acc_metric = metrics.CategoricalAccuracy()\n",
    "softmax = layers.Activation('softmax')\n",
    "\n",
    "for layer in topic_model.layers:\n",
    "    layer.trainable = True\n",
    "topic_model.layers[1].trainable = False\n",
    "topic_model.layers[-1].trainable = False\n",
    "\n",
    "@tf.function\n",
    "def train_step(x_in, y_in, x_out=None, thres=None):\n",
    "    #tf.keras.applications.inception_v3.preprocess_input(x_in)\n",
    "    f_in = feature_model(x_in)\n",
    "\n",
    "    obj_terms = {} # terms in the objective function\n",
    "    with tf.GradientTape() as tape:\n",
    "        f_in_recov, logits_in, topic_vec_n = topic_model(f_in, training=True)\n",
    "        pred_in = softmax(logits_in) # class prediction using concept scores\n",
    "\n",
    "        # total loss\n",
    "        CE_IN = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_in, pred_in))\n",
    "        loss = CE_IN\n",
    "        obj_terms['[ID] CE'] = CE_IN\n",
    "    \n",
    "    train_acc_metric.update_state(y_in, logits_in)\n",
    "    print(obj_terms)\n",
    "\n",
    "    # calculate the gradients using our tape and then update the model weights\n",
    "    print(topic_model.trainable_variables)\n",
    "    grads = tape.gradient(loss, topic_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, topic_model.trainable_variables))\n",
    "    #input()\n",
    "    return obj_terms\n",
    "\n",
    "os.makedirs(args.logdir, exist_ok=True)\n",
    "df_obj_terms = pd.DataFrame()\n",
    "for epoch in range(20):\n",
    "    logger.info(f\"\\n[INFO] starting epoch {epoch}/{20} ---------------------------------\")\n",
    "    sys.stdout.flush()\n",
    "    epochStart = time.time()\n",
    "    \n",
    "    for step, (x_in, y_in) in enumerate(train_loader):\n",
    "        \n",
    "        step += 1 # starts from 1\n",
    "        if step > len(train_loader):\n",
    "            break\n",
    "        obj_terms = train_step(x_in, y_in)\n",
    "\n",
    "        # Log every 50 batches\n",
    "        if step % 20 == 0:\n",
    "            #print(topic_model.layers[0].get_weights()[0])\n",
    "            for term in obj_terms:\n",
    "                logger.info(f'[STEP{step}] {term}: {obj_terms[term]}')\n",
    "        for term in obj_terms:\n",
    "            obj_terms[term] = obj_terms[term].numpy()\n",
    "        obj_terms[\"epoch\"] = epoch\n",
    "        obj_terms[\"step\"] = step\n",
    "        df_obj = pd.Series(obj_terms)\n",
    "        df_obj_terms = pd.concat([df_obj_terms, pd.DataFrame(df_obj).T], axis=0)\n",
    "    \n",
    "    train_acc = train_acc_metric.result()\n",
    "    logger.info(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "    \n",
    "    # show timing information for the epoch\n",
    "    epochEnd = time.time()\n",
    "    elapsed = (epochEnd - epochStart) / 60.0\n",
    "    logger.info(\"Time taken: %.2f minutes\" % (elapsed))\n",
    "\n",
    "    df_obj_terms = df_obj_terms.reset_index(drop=True)\n",
    "    df_obj_terms_melt = pd.melt(df_obj_terms, id_vars=[\"epoch\", \"step\"], \n",
    "                                value_vars=[col for col in df_obj_terms.columns if col in \n",
    "                                            ['[ID] CE']],\n",
    "                                var_name=\"loss_term\", value_name=\"loss_value\")\n",
    "\n",
    "    plt.figure()\n",
    "    sns.lineplot(data=df_obj_terms_melt, x=\"epoch\", y=\"loss_value\", hue=\"loss_term\")\n",
    "    plt.savefig(args.logdir+\"/model_compl_finetune_loss.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc_metric.reset_state()\n",
    "    topic_model.save_weights(os.path.join(args.logdir, args.name,'topic_compl_finetune_epoch{}.weights.h5'.format(epoch)))\n",
    "\n",
    "    _, logits_val, _ = topic_model(feature_model.predict(val_loader), training=False)\n",
    "    val_acc_metric.update_state(y_val_, logits_val)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    logger.info(\"[EPOCH %d] Validation acc: %.4f\" % (epoch, float(val_acc)))\n",
    "    val_acc_metric.reset_state()\n",
    "    del logits_val\n",
    "\n",
    "    _, logits_test, _ = topic_model(feature_model.predict(in_loader), training=False)\n",
    "    test_acc_metric.update_state(y_test_, logits_test)\n",
    "    test_acc = test_acc_metric.result()\n",
    "    logger.info(\"[EPOCH %d] Test acc: %.4f\" % (epoch, float(test_acc)))\n",
    "    test_acc_metric.reset_state()\n",
    "    del logits_test\n",
    "\n",
    "    logger.flush()\n",
    "\n",
    "for layer in topic_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a913e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute completeness scores\n",
    "_, logits_recov, _ = topic_model(in_test_features)\n",
    "in_test_yhat_recov = tf.math.argmax(logits_recov, axis=1).numpy()\n",
    "compute_completeness(y_test, in_test_yhat, in_test_yhat_recov, N_CLASSES, logger)\n",
    "# compute_detection_completeness(auroc, auroc_recov, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6d1a9a-65c2-439e-9f2f-17fe2c08c9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
