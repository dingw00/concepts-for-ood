{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a5fff7-c250-47e3-b15d-a401595aa482",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be17842f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import concept_model\n",
    "import helper\n",
    "from utils.log import setup_logger\n",
    "from utils.ood_utils import run_ood_over_batch\n",
    "from utils.test_utils import get_measures\n",
    "# from test_baselines import run_eval\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "import tensorflow.keras.utils as utils\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "from utils.ood_utils import run_ood_over_batch\n",
    "from utils.test_utils import get_measures\n",
    "from utils.stat_utils import multivar_separa \n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "try:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass\n",
    "\n",
    "print(tf.config.experimental.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875fb141-603e-4c16-97c9-acffec3a9617",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS:\n",
    "    def __init__(self):\n",
    "        self.gpu = \"0\"\n",
    "        self.batch_size = 256\n",
    "        self.epoch = 23\n",
    "        self.opt = \"adam\"\n",
    "        self.thres = 0\n",
    "        self.val_step = 2\n",
    "        self.save_step = 1\n",
    "        self.offset = 17\n",
    "        self.trained = False\n",
    "        self.num_concepts = 100\n",
    "\n",
    "        self.coeff_concept = 10\n",
    "        self.feat_l2 = False\n",
    "        self.coeff_feat = 0.1\n",
    "        self.feat_cosine = False\n",
    "        self.coeff_cosine = 1\n",
    "        self.ood = False\n",
    "        self.score = None # \"energy\"\n",
    "        self.coeff_score = 1\n",
    "        self.separability = False\n",
    "        self.coeff_separa = 50\n",
    "\n",
    "        self.num_hidden = 2\n",
    "\n",
    "        self.out_data = \"MSCOCO\" # \"augAwA\"\n",
    "        self.temperature_odin = 1000\n",
    "        self.epsilon_odin = 0.0\n",
    "        self.temperature_energy = 1\n",
    "        \n",
    "        self.name = \"AwA2_2_baseline_s0\" # AwA2_baseline, AwA2_feat_l2_0.1_ood_1_sep_50\n",
    "        self.logdir = \"results/\"+self.name+\"/train_logs\"\n",
    "\n",
    "args = ARGS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44281439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(bs, ood=True):\n",
    "    \"\"\"\n",
    "    prepare data loaders for ID and OOD data (train/test)\n",
    "    :param bs: batch size\n",
    "    :ood: whether to load OOD data as well (False for baseline concept learning by Yeh et al.)\n",
    "    \"\"\"\n",
    "\n",
    "    TRAIN_DIR = \"data/AwA2/train\"\n",
    "    VAL_DIR = \"data/AwA2/val\"\n",
    "    TEST_DIR = \"data/AwA2/test\"\n",
    "    if args.out_data == 'MSCOCO':\n",
    "        OOD_DIR = \"data/MSCOCO\"\n",
    "    elif args.out_data == 'augAwA':\n",
    "        OOD_DIR = \"data/AwA2-train-fractals\"\n",
    "\n",
    "    TARGET_SIZE = (224, 224)\n",
    "    BATCH_SIZE = bs\n",
    "    BATCH_SIZE_OOD = bs\n",
    "\n",
    "    print('Loading images through generators ...')\n",
    "    train_datagen = ImageDataGenerator(rescale=1. / 255.,\n",
    "                                       rotation_range=40,\n",
    "                                       width_shift_range=0.2,\n",
    "                                       height_shift_range=0.2,\n",
    "                                       shear_range=0.2,\n",
    "                                       zoom_range=0.2,\n",
    "                                       horizontal_flip=True)\n",
    "    train_loader = train_datagen.flow_from_directory(TRAIN_DIR,\n",
    "                                                    batch_size=BATCH_SIZE,\n",
    "                                                    target_size=TARGET_SIZE,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=True)\n",
    "\n",
    "    #print(train_generator.class_indices.items())\n",
    "\n",
    "    datagen = ImageDataGenerator(rescale=1.0 / 255.)\n",
    "    val_loader = datagen.flow_from_directory(VAL_DIR,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            target_size=TARGET_SIZE,\n",
    "                                            class_mode='categorical',\n",
    "                                            shuffle=False)\n",
    "    test_loader = datagen.flow_from_directory(TEST_DIR,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            target_size=TARGET_SIZE,\n",
    "                                            class_mode='categorical',\n",
    "                                            shuffle=False)\n",
    "    if ood:\n",
    "        #numUpdates = int(NUM_TRAIN / BATCH_SIZE) # int(f_train.shape[0] / BATCH_SIZE)\n",
    "        #NUM_OOD = 31706\n",
    "        #BATCH_SIZE_OOD = int(NUM_OOD / numUpdates)\n",
    "        OOD_loader = train_datagen.flow_from_directory(OOD_DIR, #datagen\n",
    "                                                batch_size=BATCH_SIZE_OOD,\n",
    "                                                target_size=TARGET_SIZE,\n",
    "                                                class_mode=None, shuffle=True)\n",
    "    else:\n",
    "        OOD_loader = None\n",
    "\n",
    "    return train_loader, val_loader, test_loader, OOD_loader\n",
    "\n",
    "\n",
    "def get_class_labels(loader, savepath):\n",
    "    \"\"\"\n",
    "    extract groundtruth class labels from data loader\n",
    "    :param loader: data loader\n",
    "    :param savepath: path to the numpy file\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(savepath):\n",
    "        y = np.load(savepath)\n",
    "    else:\n",
    "        num_data = len(loader.filenames)\n",
    "        y = []\n",
    "        for (_, y_batch), _ in zip(loader, range(len(loader))):\n",
    "            y.extend(y_batch)\n",
    "       \n",
    "        np.save(savepath, y)\n",
    "    return y\n",
    "\n",
    "def run_eval(feature_model, predict_model, in_loader, out_loader, logger, args, num_classes):\n",
    "    in_scores = np.array([])\n",
    "    for i, (x, y) in enumerate(in_loader):\n",
    "        if i == len(in_loader):\n",
    "            break\n",
    "        score = run_ood_over_batch(x, feature_model, predict_model, args, num_classes).numpy()\n",
    "        in_scores = np.concatenate([in_scores, score])\n",
    "    out_scores = np.array([])\n",
    "    for i, x in enumerate(out_loader):\n",
    "        if i == len(in_loader):\n",
    "            break\n",
    "        score = run_ood_over_batch(x, feature_model, predict_model, args, num_classes).numpy()\n",
    "        out_scores = np.concatenate([out_scores, score])\n",
    "    in_examples = np.expand_dims(in_scores, axis=1)\n",
    "    out_examples = np.expand_dims(out_scores, axis=1)\n",
    "    auroc, aupr_in, aupr_out, fpr, thres95 = get_measures(in_examples, out_examples)\n",
    "    return in_scores, out_scores, auroc, fpr, thres95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a0aac70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingw/.conda/envs/adv_train/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling2d            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,850</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_2 (\u001b[38;5;33mInputLayer\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling2d            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m524,544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m12,850\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">537,394</span> (2.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m537,394\u001b[0m (2.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">537,394</span> (2.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m537,394\u001b[0m (2.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "\n",
    "#if not os.path.exists(args.output_dir):\n",
    "#    os.makedirs(args.output_dir)\n",
    "\n",
    "if args.separability:\n",
    "    args.ood = True\n",
    "USE_OOD = args.ood\n",
    "BATCH_SIZE = args.batch_size\n",
    "EPOCH = args.epoch\n",
    "THRESHOLD = args.thres\n",
    "trained = args.trained\n",
    "N_CONCEPT = args.num_concepts\n",
    "offset = args.offset\n",
    "topic_modelpath = os.path.join(args.logdir, args.name,'topic_epoch{}.weights.h5'.format(offset))\n",
    "#topic_modelpath = os.path.join(args.logdir, args.name,'topic_latest.h5')\n",
    "topic_savepath = os.path.join(args.logdir, args.name,'topic_vec_inceptionv3.npy')\n",
    "\n",
    "logger = setup_logger(args)\n",
    "\n",
    "train_loader, val_loader, test_loader, ood_loader =  get_data(BATCH_SIZE, ood=USE_OOD)\n",
    "\n",
    "#print(train_generator.class_indices.items())\n",
    "#assert ('_OOD', 0) in val_generator.class_indices.items()\n",
    "#y_train = get_class_labels(train_loader, savepath='data/Animals_with_Attributes2/y_train.npy')\n",
    "y_val = get_class_labels(val_loader, savepath='data/AwA2/y_val.npy')\n",
    "y_test = get_class_labels(test_loader, savepath='data/AwA2/y_test.npy')\n",
    "\n",
    "# preds_cls_idx = y_test.argmax(axis=-1)\n",
    "# idx_to_cls = {v: k for k, v in test_generator.class_indices.items()}\n",
    "# preds_cls = np.vectorize(idx_to_cls.get)(preds_cls_idx)\n",
    "# filenames_to_cls = list(zip(test_generator.filenames, preds_cls))\n",
    "\n",
    "\n",
    "# Loads model\n",
    "feature_model, predict_model = helper.load_model_inception_new(train_loader, val_loader, \\\n",
    "           batch_size=BATCH_SIZE, input_size=(224,224), pretrain=True, \\\n",
    "           modelname='./results/AwA2/inceptionv3_AwA2_e40.weights.h5', split_idx=-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b12a85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concept Learning\n",
    "x, _ = test_loader.__next__()\n",
    "f = feature_model(x[:10])\n",
    "# topic model: intermediate feature --> concept score --> recovered feature --> prediction (50 classes)\n",
    "topic_model_pr = concept_model.TopicModel(f, N_CONCEPT, THRESHOLD, predict_model, args.num_hidden)\n",
    "_ = topic_model_pr(f)\n",
    "print(topic_model_pr.build_graph(f).summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27460552",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concept Learning\n",
    "x, _ = test_loader.__next__()\n",
    "f = feature_model(x[:10])\n",
    "# topic model: intermediate feature --> concept score --> recovered feature --> prediction (50 classes)\n",
    "topic_model_pr = concept_model.TopicModel(f, N_CONCEPT, THRESHOLD, predict_model, args.num_hidden)\n",
    "_ = topic_model_pr(f)\n",
    "# print(topic_model_pr.summary())\n",
    "\n",
    "if args.opt =='sgd':\n",
    "    \"\"\"\n",
    "    optimizer = SGD(lr=0.1)\n",
    "    optimizer_state = [optimizer.iterations, optimizer.lr, optimizer.momentum, optimizer.decay]\n",
    "    optimizer_reset = tf.compat.v1.variables_initializer(optimizer_state)\n",
    "    \"\"\"\n",
    "    optimizer = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "elif args.opt =='adam':\n",
    "    optimizer = Adam(learning_rate=0.01)\n",
    "    optimizer_state = [optimizer.iterations, optimizer.learning_rate, optimizer.beta_1, optimizer.beta_2, optimizer.weight_decay]\n",
    "    optimizer_reset = tf.compat.v1.variables_initializer(optimizer_state)\n",
    "\n",
    "train_acc_metric = keras.metrics.CategoricalAccuracy()\n",
    "val_acc_metric = keras.metrics.CategoricalAccuracy()\n",
    "test_acc_metric = keras.metrics.CategoricalAccuracy()\n",
    "softmax = layers.Activation('softmax')\n",
    "\n",
    "@tf.function\n",
    "def train_step(x_in, y_in, x_out=None, thres=None):\n",
    "    #tf.keras.applications.inception_v3.preprocess_input(x_in)\n",
    "    f_in = feature_model(x_in)\n",
    "    f_in_n = K.l2_normalize(f_in,axis=(3))\n",
    "\n",
    "\n",
    "    obj_terms = {} # terms in the objective function\n",
    "    COEFF_CONCEPT = args.coeff_concept #10 -> 5 -> 1 \n",
    "    with tf.GradientTape() as tape:\n",
    "        f_in_recov, logits_in, topic_vec_n = topic_model_pr(f_in, training=True)\n",
    "        pred_in = softmax(logits_in) # class prediction using concept scores\n",
    "        topic_prob_in_n = K.dot(f_in_n, topic_vec_n) # normalized concept scores\n",
    "\n",
    "        # total loss\n",
    "        CE_IN = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_in, pred_in))\n",
    "        loss_coherency = tf.reduce_mean(tf.nn.top_k(K.transpose(K.reshape(topic_prob_in_n,(-1,N_CONCEPT))),k=10,sorted=True).values)\n",
    "        loss_similarity = tf.reduce_mean(K.dot(K.transpose(topic_vec_n), topic_vec_n) - tf.eye(N_CONCEPT))\n",
    "        loss = CE_IN - COEFF_CONCEPT*loss_coherency + COEFF_CONCEPT*loss_similarity  # baseline: Yeh et al.\n",
    "        obj_terms['[ID] CE'] = CE_IN\n",
    "        obj_terms['[ID] concept coherency'] = loss_coherency\n",
    "        obj_terms['[ID] concept similarity'] = loss_similarity\n",
    "        #print('y_in: '+type(y_in).__name__)\n",
    "        #print('pred_in: '+type(pred_in).__name__)\n",
    "        #print('CE_IN: '+type(CE_IN).__name__)\n",
    "        #print('loss coher: '+type(loss_coherency).__name__)\n",
    "        #print('loss_sim: '+type(loss_similarity).__name__)\n",
    "        #print('loss: '+type(loss).__name__)\n",
    "        \n",
    "        if args.feat_l2:\n",
    "            loss_l2 = tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.pow(f_in-f_in_recov,2), axis=(1,2,3))))\n",
    "            #loss_l2 = tf.reduce_mean(tf.reduce_sum(tf.pow(f_in-f_in_recov,2), axis=(1,2,3)))\n",
    "            loss += args.coeff_feat*loss_l2 #0.07, 0.02\n",
    "            obj_terms['feature L2'] = loss_l2\n",
    "\n",
    "        if args.feat_cosine:\n",
    "            loss_cosine = tf.reduce_mean(tf.keras.losses.cosine_similarity(f_in, f_in_recov)) # equivalent to: tf.reduce_mean(tf.reduce_sum(tf.math.multiply(f_in, f_in_recov),axis=(1,2,3))/(tf.sqrt(tf.reduce_sum(tf.pow(f_in,2),axis=(1,2,3)))*tf.sqrt(tf.reduce_sum(tf.pow(f_in_recov,2),axis=(1,2,3)))))\n",
    "            loss_cosine = 1 - loss_cosine # cosine distance, range=[0, 2]\n",
    "            loss += args.coeff_cosine*loss_cosine\n",
    "            obj_terms['feature cosine distance'] = loss_cosine\n",
    "        \n",
    "        if args.score:\n",
    "            s_in = run_ood_over_batch(x_in, feature_model, predict_model, args, num_classes=50)\n",
    "            s_out = run_ood_over_batch(x_out, feature_model, predict_model, args, num_classes=50)\n",
    "\n",
    "            if args.coeff_score > 0.0:\n",
    "                # scores from OOD detector when using recovered features\n",
    "                s_in_recov = run_ood_over_batch(x_in, feature_model, topic_model_pr, args, num_classes=50)\n",
    "                s_out_recov = run_ood_over_batch(x_out, feature_model, topic_model_pr, args, num_classes=50)\n",
    "\n",
    "                s_original = tf.concat((s_in, s_out), axis=0)\n",
    "                s_recovered = tf.concat((s_in_recov, s_out_recov), axis=0)\n",
    "                loss_score = tf.reduce_mean(tf.pow(s_original - s_recovered, 2))\n",
    "                loss += args.coeff_score*loss_score\n",
    "                obj_terms['score difference'] = loss_score\n",
    "\n",
    "                \"\"\"\n",
    "                # Debugging\n",
    "                auroc, aupr_in, aupr_out, fpr95, thres95 = get_measures(s_in.numpy()[:,None], s_out.numpy()[:,None])\n",
    "                print(f'auroc: {auroc}, aupr in: {aupr_in}, aupr out: {aupr_out}, fpr95: {fpr95}')\n",
    "                auroc, aupr_in, aupr_out, fpr95, thres95 = get_measures(s_in_recov.numpy()[:,None], s_out_rec\n",
    "ov.numpy()[:,None])\n",
    "                print(f'auroc: {auroc}, aupr in: {aupr_in}, aupr out: {aupr_out}, fpr95: {fpr95}')\n",
    "                input()\n",
    "                \"\"\"\n",
    "        \n",
    "        if args.separability:\n",
    "            f_out = feature_model(x_out)\n",
    "            f_out_n = K.l2_normalize(f_out,axis=(3))\n",
    "            _, logits_out, _ = topic_model_pr(f_out, training=True)\n",
    "            #tf.debugging.assert_equal(topic_vec_n, topic_vec_n_out) \n",
    "            topic_prob_out_n = K.dot(f_out_n, topic_vec_n)\n",
    "            \n",
    "\n",
    "            # max --> smoothly approximated by logsumexp\n",
    "            #T = tf.Variable(1e+3, dtype=tf.float32)\n",
    "            T = 1e+3\n",
    "            prob_max_in = 1/T*tf.math.reduce_logsumexp(T*topic_prob_in_n,axis=(1,2))\n",
    "            prob_min_in = -1/T*tf.math.reduce_logsumexp(-T*topic_prob_in_n,axis=(1,2))\n",
    "\n",
    "            ## concept scores of \"true\" ID set and \"true\" OOD set\n",
    "            concept_in_true = tf.where(tf.abs(prob_max_in) > tf.abs(prob_min_in), prob_max_in, prob_min_in)\n",
    "            prob_max_out = 1/T*tf.math.reduce_logsumexp(T*topic_prob_out_n,axis=(1,2))\n",
    "            prob_min_out = -1/T*tf.math.reduce_logsumexp(-T*topic_prob_out_n,axis=(1,2))\n",
    "            concept_out_true = tf.where(tf.abs(prob_max_out) > tf.abs(prob_min_out), prob_max_out, prob_min_out)\n",
    "            \n",
    "            ## concept scores of \"detected\" ID set and \"detected\" OOD set\n",
    "            concept_in = tf.concat([concept_in_true[s_in>=thres], concept_out_true[s_out>=thres]], axis=0) \n",
    "            concept_out = tf.concat([concept_in_true[s_in<thres], concept_out_true[s_out<thres]], axis=0)\n",
    "\n",
    "            # global separability\n",
    "            loss_separa = multivar_separa(concept_in, concept_out)\n",
    "            loss -= args.coeff_separa*loss_separa\n",
    "            obj_terms['separability'] = loss_separa\n",
    "\n",
    "    obj_terms['total loss.......'] = loss\n",
    "    train_acc_metric.update_state(y_in, logits_in)\n",
    "    #print(obj_terms)\n",
    "\n",
    "    # calculate the gradients using our tape and then update the model weights\n",
    "    grads = tape.gradient(loss, topic_model_pr.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, topic_model_pr.trainable_variables))\n",
    "    #print(type(loss).__name__, \":\", grads)\n",
    "    #input()\n",
    "    return obj_terms\n",
    "\n",
    "\n",
    "if os.path.exists(topic_modelpath):\n",
    "    topic_model_pr.load_weights(topic_modelpath)\n",
    "    logger.info(f'topic model loaded from {topic_modelpath}')\n",
    "if not trained:\n",
    "    for layer in topic_model_pr.layers[:-1]:\n",
    "        #print(layer.trainable)\n",
    "        layer.trainable = True\n",
    "\n",
    "    # check all weights are included in trainable_variables\n",
    "    # for i, var in enumerate(topic_model_pr.trainable_variables):\n",
    "    #     print(topic_model_pr.trainable_variables[i].name)\n",
    "\n",
    "\n",
    "    if args.score and args.separability: # identify threshold from held-out set\n",
    "        datagen = ImageDataGenerator(rescale=1.0 / 255.)\n",
    "        if args.out_data == 'MSCOCO':\n",
    "            out_gen = datagen.flow_from_directory('data/MSCOCO/test',batch_size=150,target_size=(224,224),class_mode=None,shuffle=False)\n",
    "        elif args.out_data == 'augAwA':\n",
    "            out_gen = datagen.flow_from_directory('data/AwA2-test-fractals',batch_size=150,target_size=(224,224),class_mode=None,shuffle=False)\n",
    "        _, _, _, _, thres = run_eval(feature_model, predict_model, val_loader, out_gen, logger, args, 50)\n",
    "        thres = float(thres)\n",
    "    else:\n",
    "        thres = None\n",
    "\n",
    "    df_obj_terms = pd.DataFrame()\n",
    "    for epoch in range(offset+1, offset+EPOCH+1):\n",
    "        logger.info(f\"\\n[INFO] starting epoch {epoch}/{offset+EPOCH} ---------------------------------\")\n",
    "        sys.stdout.flush()\n",
    "        epochStart = time.time()\n",
    "        \n",
    "        for step, (x_in, y_in) in enumerate(train_loader):\n",
    "            \n",
    "            step += 1 # starts from 1\n",
    "            if step > len(train_loader):\n",
    "                break\n",
    "\n",
    "            if USE_OOD:\n",
    "                x_out = ood_loader.__next__()\n",
    "                obj_terms = train_step(x_in, y_in, x_out, thres)\n",
    "            else:\n",
    "                obj_terms = train_step(x_in, y_in)\n",
    "\n",
    "            # Log every 50 batches\n",
    "            if step % 20 == 0:\n",
    "                #print(topic_model_pr.layers[0].get_weights()[0])\n",
    "                for term in obj_terms:\n",
    "                    logger.info(f'[STEP{step}] {term}: {obj_terms[term]}')\n",
    "            for term in obj_terms:\n",
    "                obj_terms[term] = obj_terms[term].numpy()\n",
    "            obj_terms[\"epoch\"] = epoch\n",
    "            obj_terms[\"step\"] = step\n",
    "            df_obj = pd.Series(obj_terms)\n",
    "            df_obj_terms = pd.concat([df_obj_terms, pd.DataFrame(df_obj).T], axis=0)\n",
    "        \n",
    "        train_acc = train_acc_metric.result()\n",
    "        logger.info(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "        \n",
    "        # show timing information for the epoch\n",
    "        epochEnd = time.time()\n",
    "        elapsed = (epochEnd - epochStart) / 60.0\n",
    "        logger.info(\"Time taken: %.2f minutes\" % (elapsed))\n",
    "\n",
    "        df_obj_terms = df_obj_terms.reset_index(drop=True)\n",
    "        df_obj_terms_melt = pd.melt(df_obj_terms, id_vars=[\"epoch\", \"step\"], \n",
    "                                    value_vars=[col for col in df_obj_terms.columns if col in \n",
    "                                                ['[ID] CE', '[ID] concept coherency', 'feature L2', \n",
    "                                                 '[ID] concept similarity', 'ood score difference', \n",
    "                                                 'id & ood separability', 'total loss']],\n",
    "                                    var_name=\"loss_term\", value_name=\"loss_value\")\n",
    "\n",
    "        plt.figure()\n",
    "        sns.lineplot(data=df_obj_terms_melt, x=\"epoch\", y=\"loss_value\", hue=\"loss_term\")\n",
    "        plt.savefig(args.logdir+\"/train_loss.png\")\n",
    "        plt.close()\n",
    "        plt.figure()\n",
    "        sns.lineplot(data=df_obj_terms_melt[(df_obj_terms_melt[\"loss_term\"]=='[ID] CE') | (df_obj_terms_melt[\"loss_term\"]=='[ID] concept coherency') | \n",
    "                                            (df_obj_terms_melt[\"loss_term\"]=='[ID] concept similarity')], \n",
    "                     x=\"epoch\", y=\"loss_value\", hue=\"loss_term\")\n",
    "        plt.savefig(args.logdir+\"/train_loss1.png\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "        # Reset training metrics at the end of each epoch\n",
    "        train_acc_metric.reset_state()\n",
    "        if epoch % args.save_step == 0:\n",
    "            topic_model_pr.save_weights(os.path.join(args.logdir, args.name,'topic_epoch{}.weights.h5'.format(epoch)))\n",
    "\n",
    "        if epoch % args.val_step == 0:\n",
    "            _, logits_val, _ = topic_model_pr(feature_model.predict(val_loader), training=False)\n",
    "            pred_val = softmax(logits_val)\n",
    "            val_acc_metric.update_state(y_val, logits_val)\n",
    "            val_acc = val_acc_metric.result()\n",
    "            logger.info(\"[EPOCH %d] Validation acc: %.4f\" % (epoch, float(val_acc)))\n",
    "            val_acc_metric.reset_state()\n",
    "            del logits_val\n",
    "        \n",
    "        logger.flush()\n",
    "\n",
    "\n",
    "topic_vec = topic_model_pr.layers[0].get_weights()[0]   # 1, (2048, num_concepts)\n",
    "# recov_vec = topic_model_pr.layers[-3].get_weights()[0]\n",
    "topic_vec_n = topic_vec/(np.linalg.norm(topic_vec,axis=0,keepdims=True)+1e-9)\n",
    "np.save(topic_savepath,topic_vec)\n",
    "# np.save('results/Animals_with_Attributes2_energy_COCO/recov_vec_inceptionv3.npy',recov_vec)\n",
    "\n",
    "assert np.shape(topic_vec)[1] == N_CONCEPT\n",
    "# topic_model_pr.evaluate(f_test, y_test)\n",
    "# f_val_recovered = topic_model_pr.predict(f_val)\n",
    "\n",
    "\n",
    "f_test = feature_model.predict(test_loader)\n",
    "_, logits_test, _ = topic_model_pr(f_test, training=False)\n",
    "pred_test = softmax(logits_test)\n",
    "test_acc_metric.update_state(y_test, logits_test)\n",
    "test_acc = test_acc_metric.result()\n",
    "logger.info('[ID TEST] Accuracy of topic model on test set: %f' %test_acc)\n",
    "\n",
    "logger.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46e4492-467c-4aca-933c-56bed65f465b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
